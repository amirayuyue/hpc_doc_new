"use strict";(self.webpackChunkhpc_doc_new=self.webpackChunkhpc_doc_new||[]).push([[8078],{742:(e,t,s)=>{s.r(t),s.d(t,{assets:()=>a,contentTitle:()=>r,default:()=>h,frontMatter:()=>i,metadata:()=>d,toc:()=>l});var n=s(4848),o=s(8453);s(6012),s(1470),s(9365),s(7293);const i={title:"Job Management",sidebar_position:3},r="Job Management",d={id:"HPC-clusters/job_manage",title:"Job Management",description:"Presented here are helpful tools and methods to manage  Slurm jobs, find detailed information of a job like memory usage, CPUs, and how to use job statistics/information to troubleshoot any job failure.",source:"@site/docs/HPC-clusters/job_manage.md",sourceDirName:"HPC-clusters",slug:"/HPC-clusters/job_manage",permalink:"/hpc_doc_new/docs/HPC-clusters/job_manage",draft:!1,unlisted:!1,editUrl:"https://github.com/amirayuyue/hpc_doc_new/docs/HPC-clusters/job_manage.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{title:"Job Management",sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"Running Jobs",permalink:"/hpc_doc_new/docs/HPC-clusters/jobs"},next:{title:"Running Common Applications",permalink:"/hpc_doc_new/docs/HPC-clusters/run_apps"}},a={},l=[{value:"Checking the use of a mixed-state node.",id:"checking-the-use-of-a-mixed-state-node",level:2},{value:"Checking jobs after submission",id:"checking-jobs-after-submission",level:2},{value:"Job State",id:"job-state",level:2},{value:"Nodelist Reasons",id:"nodelist-reasons",level:2},{value:"<code>sacct</code> command",id:"sacct-command",level:2},{value:"<code>scontrol</code> Command",id:"scontrol-command",level:2},{value:"Common Issues  ",id:"common-issues--",level:2},{value:"Out of Memory Issues ",id:"out-of-memory-issues-",level:3},{value:"Time-Out Issues ",id:"time-out-issues-",level:3},{value:"Useful proccess to follow to ensure sucessful completion of jobs ",id:"useful-proccess-to-follow-to-ensure-sucessful-completion-of-jobs-",level:2},{value:"Other Useful Commands  ",id:"other-useful-commands--",level:2}];function c(e){const t={admonition:"admonition",blockquote:"blockquote",br:"br",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",header:"header",hr:"hr",li:"li",mdxAdmonitionTitle:"mdxAdmonitionTitle",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",...(0,o.R)(),...e.components},{Details:s}=t;return s||function(e,t){throw new Error("Expected "+(t?"component":"object")+" `"+e+"` to be defined: you likely forgot to import, pass, or provide it.")}("Details",!0),(0,n.jsxs)(n.Fragment,{children:[(0,n.jsx)(t.header,{children:(0,n.jsx)(t.h1,{id:"job-management",children:"Job Management"})}),"\n",(0,n.jsx)(t.p,{children:"Presented here are helpful tools and methods to manage  Slurm jobs, find detailed information of a job like memory usage, CPUs, and how to use job statistics/information to troubleshoot any job failure."}),"\n",(0,n.jsx)(t.h2,{id:"checking-the-use-of-a-mixed-state-node",children:"Checking the use of a mixed-state node."}),"\n",(0,n.jsx)(t.p,{children:"A mixed state node is a node that is not being fully utilized, i.e. their resources are not fully allocated.\nExecute the following script on the login node to get detailed information about memory and cpu core availabilty on every currently, mixed-state node on Pinnacles."}),"\n",(0,n.jsxs)(t.p,{children:["Execute the script via the command:\n",(0,n.jsx)(t.code,{children:"node_resource_status.sh"})]}),"\n",(0,n.jsx)(t.h2,{id:"checking-jobs-after-submission",children:"Checking jobs after submission"}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.code,{children:"squeue"})," is a  command that can check the state and workload of the overall cluster as well as more specific information. Below is a table of options that can be added to view certain information. By default ",(0,n.jsx)(t.code,{children:"squeue"})," will show all currently submitted and running jobs on Pinnacles."]}),"\n",(0,n.jsxs)(t.table,{children:[(0,n.jsx)(t.thead,{children:(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.th,{children:"Command_Option"}),(0,n.jsx)(t.th,{children:"Use"})]})}),(0,n.jsxs)(t.tbody,{children:[(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:(0,n.jsx)(t.code,{children:"-M merced "})}),(0,n.jsx)(t.td,{children:"Shows all currently submitted jobs on MERCED"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:(0,n.jsx)(t.code,{children:"--me "})}),(0,n.jsx)(t.td,{children:"Shows all currently jobs submitted by user"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsxs)(t.td,{children:[(0,n.jsx)(t.code,{children:"--r"})," or ",(0,n.jsx)(t.code,{children:"-array "})]}),(0,n.jsx)(t.td,{children:"Shows job arrays sumitted onto cluster"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:(0,n.jsx)(t.code,{children:"--start"})}),(0,n.jsx)(t.td,{children:"Shows rough estimate of when jobs for specified user will begin, based off real-time state of the scheduler and jobs queued. Not always accurate estimates."})]})]})]}),"\n",(0,n.jsx)(t.admonition,{type:"tip",children:(0,n.jsxs)(t.p,{children:["Flags can be used together in the same line for example: ",(0,n.jsx)(t.code,{children:"squeue -M merced --me --start"})]})}),"\n",(0,n.jsx)(t.h2,{id:"job-state",children:"Job State"}),"\n",(0,n.jsx)(t.p,{children:"Job states are the current state of the jobs that were submitted. Some important state codes that are useful are given below:"}),"\n",(0,n.jsxs)(t.table,{children:[(0,n.jsx)(t.thead,{children:(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.th,{children:"State Codes"}),(0,n.jsx)(t.th,{children:"Meaning"})]})}),(0,n.jsxs)(t.tbody,{children:[(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"PD"}),(0,n.jsx)(t.td,{children:"Job is Pending"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"R"}),(0,n.jsx)(t.td,{children:"Running"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"CF"}),(0,n.jsx)(t.td,{children:"Job is resources allocated and now booting up"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"CD"}),(0,n.jsx)(t.td,{children:"Job has been completed"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"CA"}),(0,n.jsx)(t.td,{children:"Job has been cancelled explicitly"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"CG"}),(0,n.jsx)(t.td,{children:"Job is in the process of completting and is deallocating the resources"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"F"}),(0,n.jsx)(t.td,{children:"Job exited with Failure, a non-zero exit code is presented"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"OOM"}),(0,n.jsx)(t.td,{children:"Node are out of memory"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"S"}),(0,n.jsx)(t.td,{children:"Job has been suspended"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"TO"}),(0,n.jsx)(t.td,{children:"Job terminated upon reaching its time limit"})]})]})]}),"\n",(0,n.jsx)(t.h2,{id:"nodelist-reasons",children:"Nodelist Reasons"}),"\n",(0,n.jsx)(t.p,{children:"NodeList(Reason) helps to find on which nodes the job is currently running on. Also, in the case of PD Job state, this field will give more information about the reason why the job is in pending state. Below is a table that shows common Nodelist(reasons) and their meanings."}),"\n",(0,n.jsxs)(t.table,{children:[(0,n.jsx)(t.thead,{children:(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.th,{children:"(Reason)"}),(0,n.jsx)(t.th,{children:"Meaning"})]})}),(0,n.jsxs)(t.tbody,{children:[(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"(Resources)"}),(0,n.jsx)(t.td,{children:"Job is waiting for resources to become available"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"(TimeLimit)"}),(0,n.jsx)(t.td,{children:"Job has hit it's max time limit for execution"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"(ReqNodeNotAvail)"}),(0,n.jsx)(t.td,{children:"The requested node is not curretly available"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"(Nodes required for job are DOWN, DRAINED or reserved for jobs in higher priority partitions)"}),(0,n.jsx)(t.td,{children:"Job is not available"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"(Priority)"}),(0,n.jsx)(t.td,{children:"One or more higher priority jobs exist for this partition or advanced reservation."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"(QoSJobLimit)"}),(0,n.jsx)(t.td,{children:"The job's QOS has reached its maximum job count"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"QOSResourceLimit"}),(0,n.jsx)(t.td,{children:"The job's QOS has reached some resource limit"})]})]})]}),"\n",(0,n.jsxs)(t.h2,{id:"sacct-command",children:[(0,n.jsx)(t.code,{children:"sacct"})," command"]}),"\n",(0,n.jsxs)(t.p,{children:["The ",(0,n.jsx)(t.code,{children:"sacct"})," displays accounting data for all jobs in the cluster queue or recent history. By default, the ",(0,n.jsx)(t.code,{children:"sacct"})," command diplays JobId, JobName, Partition, Account, AllocCPUS, State and ExitCode. Below are useful options that can be added to get more specific information but all options for ",(0,n.jsx)(t.code,{children:"sacct "})," can be found through executing ",(0,n.jsx)(t.code,{children:"sacct -e"})," or ",(0,n.jsx)(t.code,{children:"sacct -h"}),"."]}),"\n",(0,n.jsxs)(t.table,{children:[(0,n.jsx)(t.thead,{children:(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.th,{children:"Option"}),(0,n.jsx)(t.th,{children:"Meaning"})]})}),(0,n.jsxs)(t.tbody,{children:[(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"--user=[uid_or_user_list]"}),(0,n.jsx)(t.td,{children:"Displays the list of jobs currently submitted and running on the cluster of the specified user."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"-j, --jobs=[JobID]"}),(0,n.jsx)(t.td,{children:"Displays information about the job ID inputted"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"-C, --constraints=[constraint_list]"}),(0,n.jsx)(t.td,{children:"Comma separated list to filter jobs based on what constraints/features the job requested. Multiple options will be treated as 'and' not 'or', so the job would need all constraints specified to be returned not one or the other."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"-h, --help"}),(0,n.jsxs)(t.td,{children:["Displays all options and descriptions for ",(0,n.jsx)(t.code,{children:"sacct"})]})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"-X, --allocations"}),(0,n.jsx)(t.td,{children:"Only show statistics relevant to the job allocation itself"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"-v, --verbose"}),(0,n.jsx)(t.td,{children:"Primarily for debugging purposes, report the state of various variables during processing."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"--name=[jobname_list]"}),(0,n.jsx)(t.td,{children:"Display jobs that have any of these name(s)."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"--state=[state_list]"}),(0,n.jsx)(t.td,{children:"Displays states depending on which state was asked to be displayed and thier associated exit code."})]})]})]}),"\n",(0,n.jsxs)(t.p,{children:["Example of ",(0,n.jsx)(t.code,{children:"sacct -e"})]}),"\n",(0,n.jsx)(s,{children:(0,n.jsxs)(t.p,{children:[(0,n.jsx)("summary",{children:"Click to see the full ouput list "}),"\nAccount             AdminComment        AllocCPUS           AllocNodes\nAllocTRES           AssocID             AveCPU              AveCPUFreq\nAveDiskRead         AveDiskWrite        AvePages            AveRSS\nAveVMSize           BlockID             Cluster             Comment\nConstraints         ConsumedEnergy      ConsumedEnergyRaw   Container\nCPUTime             CPUTimeRAW          DBIndex             DerivedExitCode\nElapsed             ElapsedRaw          Eligible            End\nExitCode            Flags               GID                 Group\nJobID               JobIDRaw            JobName             Layout\nMaxDiskRead         MaxDiskReadNode     MaxDiskReadTask     MaxDiskWrite\nMaxDiskWriteNode    MaxDiskWriteTask    MaxPages            MaxPagesNode\nMaxPagesTask        MaxRSS              MaxRSSNode          MaxRSSTask\nMaxVMSize           MaxVMSizeNode       MaxVMSizeTask       McsLabel\nMinCPU              MinCPUNode          MinCPUTask          NCPUS\nNNodes              NodeList            NTasks              Partition\nPriority            QOS                 QOSRAW              Reason\nReqCPUFreq          ReqCPUFreqGov       ReqCPUFreqMax       ReqCPUFreqMin\nReqCPUS             ReqMem              ReqNodes            ReqTRES\nReservation         ReservationId       Reserved            ResvCPU\nResvCPURAW          Start               State               Submit\nSubmitLine          Suspended           SystemComment       SystemCPU\nTimelimit           TimelimitRaw        TotalCPU            TRESUsageInAve\nTRESUsageInMax      TRESUsageInMaxNode  TRESUsageInMaxTask  TRESUsageInMin\nTRESUsageInMinNode  TRESUsageInMinTask  TRESUsageInTot      TRESUsageOutAve\nTRESUsageOutMax     TRESUsageOutMaxNode TRESUsageOutMaxTask TRESUsageOutMin\nTRESUsageOutMinNode TRESUsageOutMinTask TRESUsageOutTot     UID\nUser                UserCPU             WCKey               WCKeyID\nWorkDir"]})}),"\n",(0,n.jsx)(t.p,{children:"Below are defintions of some important fields from the above list that are helpful when troubleshooting or debugging."}),"\n",(0,n.jsxs)(t.table,{children:[(0,n.jsx)(t.thead,{children:(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.th,{children:"Field"}),(0,n.jsx)(t.th,{children:"Use"})]})}),(0,n.jsxs)(t.tbody,{children:[(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"JobId"}),(0,n.jsx)(t.td,{children:"Shows the ID of the job"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"JobName"}),(0,n.jsx)(t.td,{children:"Name of the Job."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"AllocCPUS"}),(0,n.jsx)(t.td,{children:"Count of allocated CPUs. Equal to NCPUS."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"ReqCPUS"}),(0,n.jsx)(t.td,{children:"Required number of CPUS."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"ReqMem"}),(0,n.jsx)(t.td,{children:"Minimum memory required for the job in MB. A c in the end denotes Memory Per CPU and a n at the end represents Memory Per Node."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"AveRSS"}),(0,n.jsx)(t.td,{children:"Average memory use of all tasks in the job."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"MaxRSS"}),(0,n.jsx)(t.td,{children:"Maximum memory use of any task in the job."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"Start"}),(0,n.jsx)(t.td,{children:"Initiation time of the job in the same format as End"})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"End"}),(0,n.jsx)(t.td,{children:"Termination time of the job."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"Elapsed"}),(0,n.jsx)(t.td,{children:"Time taken by the job."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"State"}),(0,n.jsx)(t.td,{children:"State of the job."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"ExitCode"}),(0,n.jsx)(t.td,{children:"Exit code returned by the job."})]})]})]}),"\n",(0,n.jsxs)(t.p,{children:["Here is one use of ",(0,n.jsx)(t.code,{children:"sacct"})," with the follwing syntax to retrieve useful information about the specified job:\n",(0,n.jsx)(t.code,{children:"sacct -j <jobid>"})]}),"\n",(0,n.jsx)(t.p,{children:"This will provide similar information with the same fields as shown below about the specified job:"}),"\n",(0,n.jsx)(t.p,{children:"JobID           JobName  Partition    Account  AllocCPUS      State ExitCode"}),"\n",(0,n.jsx)(t.hr,{}),"\n",(0,n.jsx)(t.p,{children:"569893         javatest       test project_u+          1 OUT_OF_ME+    0:125\n569893.batch      batch            project_u+          1 OUT_OF_ME+    0:125\n569893.exte+     extern            project_u+          1  COMPLETED      0:0"}),"\n",(0,n.jsx)(t.p,{children:'By default, sacct -j [jobid] displays basic job information such as job ID, job name, partition, allocated CPUs, state, and exit code. This is helpful for debugging job failures. In the example, the "javatest" job ran in the test partition with 1 CPU but ended with OUT_OF_ME+, indicating an incomplete message due to truncation. The exit code 0:125 confirms the job ran out of memory.'}),"\n",(0,n.jsxs)(t.p,{children:["For debugging that requires more in-depth analysis and information adding the option ",(0,n.jsx)(t.code,{children:"--format=<Field>"})," will show additional information that can be more useful for debugging bigger issues. Below is an example with the use of ",(0,n.jsx)(t.code,{children:"--format=<Field"}),"."]}),"\n",(0,n.jsx)(t.p,{children:"Using sacct -j [jobid] --format=jobid,jobname,reqcpus,reqmem,averss,maxrss,elapsed,state,exitcode, you can gain more detailed insights into job performance and failures."}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{children:"JobID           JobName  ReqCPUS     ReqMem     AveRSS     MaxRSS    Elapsed                State ExitCode\n------------ ---------- -------- ---------- ---------- ---------- ---------- -------------------- --------\n569966          testjob        1         1M                         00:00:06        OUT_OF_MEMORY    0:125\n569966.batch      batch        1                 1312K      1312K   00:00:06        OUT_OF_MEMORY    0:125\n569966.exte+     extern        1                  916K       916K   00:00:06            COMPLETED      0:0\n"})}),"\n",(0,n.jsxs)(t.p,{children:["Refer to the above table for details about each field's purpose and how it aids in diagnosing job failures. By adding more fields with ",(0,n.jsx)(t.code,{children:"--format="}),", you can perform deeper analysis on when, how, and why a job failed or ran into issues; significantly speeding up the debugging process."]}),"\n",(0,n.jsxs)(t.h2,{id:"scontrol-command",children:[(0,n.jsx)(t.code,{children:"scontrol"})," Command"]}),"\n",(0,n.jsxs)(t.p,{children:[(0,n.jsx)(t.code,{children:"scontrol"})," is a helpful command that allows to  view or configure the submitted job and it's state. scontrol is used to view or modify Slurm configuration including: job, job step, node, partition, reservation, and overall system configuration. If no command is entered on the execute line, scontrol will operate in an interactive mode and prompt for input. It will continue prompting for input and executing commands until explicitly terminated."]}),"\n",(0,n.jsxs)(t.p,{children:["Use ",(0,n.jsx)(t.code,{children:"scontrol"})," with the follwing syntax to retrieve useful information about the specified job:"]}),"\n",(0,n.jsx)(t.p,{children:(0,n.jsx)(t.code,{children:"scontrol show job <jobid>"})}),"\n",(0,n.jsxs)(t.p,{children:["Below is an example of using scontrol to get insight about an example job. An example job is running and has not yet terminated. It is shown as ",(0,n.jsx)(t.code,{children:"JobState=RUNNING Reason=NONE"})," & ",(0,n.jsx)(t.code,{children:"ExitCod=0:0"}),"."]}),"\n",(0,n.jsx)(s,{children:(0,n.jsxs)(t.p,{children:[(0,n.jsx)("summary",{children:" Sample Output "}),"\nUserId=guest001 GroupId=****** MCS_label=N/A\nPriority=4294341021 Nice=0 Account=project_**** QOS=normal\nJobState=RUNNING Reason=None Dependency=(null)\nRequeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:0\nRunTime=00:02:33 TimeLimit=00:15:00 TimeMin=N/A\nSubmitTime=2023-07-19T12:50:52 EligibleTime=2023-07-19T12:50:52\nAccrueTime=2023-07-19T12:50:52\nStartTime=2023-07-19T12:50:53 EndTime=2023-07-19T13:05:53 Deadline=N/A\nSuspendTime=None SecsPreSuspend=0 LastSchedEval=2023-07-19T12:50:53 Scheduler=Main\nPartition=test AllocNode",":Sid","=10.1.2.252:279163\nReqNodeList=(null) ExcNodeList=(null)\nNodeList=hmnode003\nBatchHost=hmnode003\nNumNodes=1 NumCPUs=1 NumTasks=1 CPUs/Task=1 ReqB:S:C",":T","=0:0:",(0,n.jsx)(t.em,{children:":"}),"\nTRES=cpu=1,mem=1M,node=1,billing=1\nSocks/Node=* NtasksPerN:B:S",":C","=0:0:",(0,n.jsx)(t.em,{children:":"})," CoreSpec=*\nMinCPUsNode=1 MinMemoryNode=1M MinTmpDiskNode=0\nFeatures=(null) DelayBoot=00:00:00\nOverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)\nCommand=/home/",(0,n.jsx)(t.strong,{children:(0,n.jsx)(t.strong,{children:(0,n.jsx)(t.strong,{children:"/testoom/job.bat\nWorkDir=/home/"})})}),"/testoom\nStdErr=/home/",(0,n.jsx)(t.strong,{children:(0,n.jsx)(t.strong,{children:(0,n.jsx)(t.strong,{children:"/testoom/Appout.qlog\nStdIn=/dev/null\nStdOut=/home/"})})}),"/testoom/Appout.qlog\nPower="]})}),"\n",(0,n.jsxs)(t.admonition,{type:"info",children:[(0,n.jsx)(t.mdxAdmonitionTitle,{}),(0,n.jsx)(t.p,{children:"Astericks are only in the above sample output to protect user information."})]}),"\n",(0,n.jsx)(t.h2,{id:"common-issues--",children:"Common Issues  "}),"\n",(0,n.jsx)(t.p,{children:"Below are common issues, that can arrise when running jobs on the clusters, and associated troubleshooting methods."}),"\n",(0,n.jsx)(t.h3,{id:"out-of-memory-issues-",children:"Out of Memory Issues "}),"\n",(0,n.jsxs)(t.p,{children:["Jobs can fail if the memory requested for the job exceeds the actual memory needed for the job to complete successfully.\nIt is good practice to always check the job state and exit code with ",(0,n.jsx)(t.code,{children:"sacct -j <JobID>"}),". It can be concluded that a job has had a ",(0,n.jsx)(t.strong,{children:"OUT_OF_MEMORY"})," error from reading the job state column and exit code. Furthermore, the output file produced by the failed job should also contain error messages that can be associated with the job running out of memory."]}),"\n",(0,n.jsx)(t.p,{children:"Below is a job script that will result in an out of memory error."}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{children:"#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH --partition test\n#SBATCH --mem=1M # This is where the issue arrises\n#SBATCH --time=0-00:15:00 # 15 minute\n#SBATCH --output=oomout.qlog\n#SBATCH --job-name=testjob\n#SBATCH --export=ALL\n\nsleep 5\nmodule load anaconda3\npython oom.py\n"})}),"\n",(0,n.jsx)(t.p,{children:"Sample python program that was used in the job sample script above is shown below:"}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{children:"import sys\nimport os\n\nmy_list = [i for i in range(1000000)]\n\nprint(my_list)\n"})}),"\n",(0,n.jsxs)(t.p,{children:["Check the status of the job using ",(0,n.jsx)(t.code,{children:"sacct -j <jobid>"})," the follwing is produced:"]}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{children:"JobID           JobName  ReqCPUS     ReqMem     AveRSS     MaxRSS    Elapsed                State ExitCode\n------------ ---------- -------- ---------- ---------- ---------- ---------- -------------------- --------\n569966          testjob        1         1M                         00:00:06        OUT_OF_MEMORY    0:125\n569966.batch      batch        1                 1312K      1312K   00:00:06        OUT_OF_MEMORY    0:125\n569966.exte+     extern        1                  916K       916K   00:00:06            COMPLETED      0:0\n"})}),"\n",(0,n.jsxs)(t.p,{children:["Using the ",(0,n.jsx)(t.code,{children:"sacct"})," command we see that the job failed because it ran out of memory. This is inferred through the state: ",(0,n.jsx)(t.code,{children:"OUT_OF_MEMORY"})," and the exit code of ",(0,n.jsx)(t.code,{children:"0:125"})," which correlates with an Out of Memory exit status or the reason why the job session was terminated."]}),"\n",(0,n.jsxs)(t.p,{children:["It is possible to use ",(0,n.jsx)(t.code,{children:"scontrol show job <sampleid>"})," to debug the error(s) that occured in our job."]}),"\n",(0,n.jsx)(t.pre,{children:(0,n.jsx)(t.code,{children:"   JobId=569966 JobName=testjob\n   UserId=**** GroupId=**** MCS_label=N/A\n   Priority=4294340955 Nice=0 Account=**** QOS=normal\n   JobState=OUT_OF_MEMORY Reason=OutOfMemory Dependency=(null)\n   Requeue=0 Restarts=0 BatchFlag=1 Reboot=0 ExitCode=0:125\n   RunTime=00:00:05 TimeLimit=00:15:00 TimeMin=N/A\n   SubmitTime=2023-07-19T15:54:36 EligibleTime=2023-07-19T15:54:36\n   AccrueTime=2023-07-19T15:54:36\n   StartTime=2023-07-19T15:54:37 EndTime=2023-07-19T15:54:42 Deadline=N/A\n   SuspendTime=None SecsPreSuspend=0 LastSchedEval=2023-07-19T15:54:37 Scheduler=Main\n   Partition=test AllocNode:Sid=rclogin01:692486\n   ReqNodeList=(null) ExcNodeList=(null)\n   NodeList=gnode009\n   BatchHost=gnode009\n   NumNodes=1 NumCPUs=1 NumTasks=1 CPUs/Task=1 ReqB:S:C:T=0:0:*:*\n   TRES=cpu=1,mem=1M,node=1,billing=1\n   Socks/Node=* NtasksPerN:B:S:C=0:0:*:* CoreSpec=*\n   MinCPUsNode=1 MinMemoryNode=1M MinTmpDiskNode=0\n   Features=(null) DelayBoot=00:00:00\n   OverSubscribe=OK Contiguous=0 Licenses=(null) Network=(null)\n   Command=/scratch/****/python_OOO/job.sub\n   WorkDir=/scratch/****/python_OOO\n   StdErr=/scratch/****/python_OOO/oomout.qlog\n   StdIn=/dev/null\n   StdOut=/scratch/****/python_OOO/oomout.qlog\n   Power=\n"})}),"\n",(0,n.jsxs)(t.p,{children:["Looking through the output of ",(0,n.jsx)(t.code,{children:"scontrol"})," we can see the job state, the state the job was last recorded at before the session was terminated or ended, was ",(0,n.jsx)(t.code,{children:"OUT_OF_MEMORY"}),", the node reason was listed at ",(0,n.jsx)(t.code,{children:"OUTofMemory"})," and the exit code was recorded at, before the job session was terminated, ",(0,n.jsx)(t.code,{children:"0:125"}),". All of these fields are useful and allow for the debugging process to conclude that the job did not succesfully run because of a memory capacity issue."]}),"\n",(0,n.jsx)(t.h3,{id:"time-out-issues-",children:"Time-Out Issues "}),"\n",(0,n.jsxs)(t.p,{children:["One common issue for jobs failing is if job does not complete in the allocated time. This leads to a ",(0,n.jsx)(t.strong,{children:"Time-Out"})," State and a ",(0,n.jsx)(t.code,{children:"(TimeLimit)"})," nodelist reason. The best approach is to increase the time being allocated for the job to run, ensuring that the job does not exceed the partition's max walltime. If the job continues to fail with a ",(0,n.jsx)(t.strong,{children:"Time-Out"})," state then it is best to break the job down into smaller jobs,  make it into a job array or change the partition that the job is being placed onto to run and compute."]}),"\n",(0,n.jsx)(t.p,{children:"Below is a script that will result in a time-out error."}),"\n",(0,n.jsx)(t.p,{children:"#!/bin/bash\n#SBATCH --nodes=1\n#SBATCH --ntasks=1\n#SBATCH -p test\n#SBATCH --mem=1G  #Here is the reason why the job fails\n#SBATCH --time=0-00:01:00     # 1 mins\n#SBATCH --output=regular.stdout\n#SBATCH --job-name=test\n#SBATCH --export=ALL"}),"\n",(0,n.jsx)(t.p,{children:'echo "Starting Process"\nsleep 180\necho "Ending Process"'}),"\n",(0,n.jsxs)(t.p,{children:['This simple job script is printing out "Starting Process" and then sleeps or waits 180 seconds before again executing the following line of printing out "Ending Process".  After submitting this script using ',(0,n.jsx)(t.code,{children:"sbatch"})," it gets placed on the requested partition and then it begins to run until it hits its max wall time of 1 minute. This job will because the mininimum time needed was 180 seconds as that is the time the job sleeps after running and is the time required at minimum for the job to fully execute."]}),"\n",(0,n.jsxs)(t.p,{children:["Using ",(0,n.jsx)(t.code,{children:"sacct -j <jobID> --format=jobid,jobname,reqcpus,reqmem,elapsed,state,exitcode"}),". We get a result table that shows that the first part timed out and thus resulted in a failed, timedout and cancelled state."]}),"\n",(0,n.jsx)(t.p,{children:"JobID           JobName  ReqCPUS     ReqMem    Elapsed      State ExitCode"}),"\n",(0,n.jsx)(t.hr,{}),"\n",(0,n.jsx)(t.p,{children:"569330             test        1         1G   00:01:02    TIMEOUT      0:0\n569330.batch      batch        1              00:01:03  CANCELLED     0:15\n569330.exte+     extern        1              00:01:02  COMPLETED      0:0\n569330.0           echo        1              00:00:00  COMPLETED      0:0\n569330.1          sleep        1              00:01:02  CANCELLED     0:15"}),"\n",(0,n.jsxs)(t.p,{children:["Using the sacct command we see that the job resulted in ",(0,n.jsx)(t.code,{children:"TIMEOUT"})," state which allows us to debug that the issue was an walltime issue issue. This can further be seen as ",(0,n.jsx)(t.code,{children:"569330.0"})," or ",(0,n.jsx)(t.code,{children:"echo"}),' on the fourth line shows that echo was completed it was able to execute in the begginging when it printed "Starting Process" but echo was never called again as the job timed-out so the second echo which prints out "Ending Process" was never reached as the job stoped one line before it.']}),"\n",(0,n.jsx)(t.p,{children:"It always important to note that sometimes a job failing not the result of one issue or error, but a combination of many errors and issues. Furthermore it is best to keep track of jobs before, during and after completion."}),"\n",(0,n.jsx)(t.h2,{id:"useful-proccess-to-follow-to-ensure-sucessful-completion-of-jobs-",children:"Useful proccess to follow to ensure sucessful completion of jobs "}),"\n",(0,n.jsxs)(t.ol,{children:["\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsx)(t.p,{children:"Submit the Job\nSubmit the above job and see how it runs. To submit the above job, run the following command."}),"\n",(0,n.jsx)(t.p,{children:"sbatch script.sh\nSubmitted batch job [JOBID]"}),"\n"]}),"\n",(0,n.jsxs)(t.li,{children:["\n",(0,n.jsxs)(t.p,{children:["Watch Live Status of the Job.\nUse the ",(0,n.jsx)(t.code,{children:"watch squeue -u <username>"}),". Do not include anything past the ",(0,n.jsx)(t.code,{children:"@"})," in the username. Ex. ",(0,n.jsx)(t.code,{children:"watch squeue -u guest001"}),". Empty Version of Live Status is below."]}),"\n",(0,n.jsxs)(t.p,{children:["Every 2.0s: squeue -u guest001\nCLUSTER: pinnacles",(0,n.jsx)(t.br,{}),"\n","JOBID PARTITION     NAME     USER ST\tTIME  NODES NODELIST(REASON)"]}),"\n"]}),"\n"]}),"\n",(0,n.jsxs)(t.blockquote,{children:["\n",(0,n.jsx)(t.p,{children:"To exit the live status of the watch squeue command, press Ctrl + C"}),"\n"]}),"\n",(0,n.jsxs)(t.ol,{start:"3",children:["\n",(0,n.jsxs)(t.li,{children:["After the job exits from the queue, run the below sacct command to check the status of the job.\n",(0,n.jsx)(t.code,{children:"sacct -j <JobID> "})," or ",(0,n.jsx)(t.code,{children:"scontrol show job <jobid>"})]}),"\n"]}),"\n",(0,n.jsx)(t.p,{children:"Sacct Command SampleOutput:"}),"\n",(0,n.jsx)(t.p,{children:"JobID    JobName    Elapsed      State ExitCode"}),"\n",(0,n.jsx)(t.hr,{}),"\n",(0,n.jsx)(t.p,{children:"568963             test        3      1024M                         00:01:10              TIMEOUT      0:0\n568963.batch      batch        3                12.01M     12.01M   00:01:11            CANCELLED     0:15\n568963.exte+     extern        3                 0.90M      0.90M   00:01:10            COMPLETED      0:0\n568963.0           echo        3                 3.30M      3.30M   00:00:00            COMPLETED      0:0\n568963.1          sleep        3                 3.27M      3.27M   00:01:12            CANCELLED     0:15"}),"\n",(0,n.jsx)(t.h2,{id:"other-useful-commands--",children:"Other Useful Commands  "}),"\n",(0,n.jsxs)(t.table,{children:[(0,n.jsx)(t.thead,{children:(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.th,{children:"Command"}),(0,n.jsx)(t.th,{children:"Use"})]})}),(0,n.jsxs)(t.tbody,{children:[(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"scancel [jobid] or skill [jobid]"}),(0,n.jsx)(t.td,{children:"These commands will kill the specified job in it's current process and state."})]}),(0,n.jsxs)(t.tr,{children:[(0,n.jsx)(t.td,{children:"seff [jobid]"}),(0,n.jsx)(t.td,{children:"This command can be used to find the job efficiency report for the job(s) after it has completed and exited from the queue. Some information in the report are: State, CPU & Memory Utilized, CPU & Memory Efficiency. If the command ussed while the job is still in the R(Running) state, this might report incorrect information."})]})]})]})]})}function h(e={}){const{wrapper:t}={...(0,o.R)(),...e.components};return t?(0,n.jsx)(t,{...e,children:(0,n.jsx)(c,{...e})}):c(e)}},6012:(e,t,s)=>{s.d(t,{A:()=>o});s(6540);var n=s(4848);function o(e){let{children:t,color:s}=e;return(0,n.jsx)("span",{style:{backgroundColor:s,borderRadius:"4px",color:"#fff",padding:"0.2rem 0.5rem",fontWeight:"bold"},children:t})}}}]);